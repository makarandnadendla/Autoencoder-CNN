{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General flow of the report \n",
    "\n",
    "1) Introduction <br>\n",
    "2) Dataset explanation<br>\n",
    "3) (Model/Architecture explanation, Experiment, Results (please include figures such as a confusion matrix)) * how ever many models you implement<br>\n",
    "4) Discussion <br>\n",
    "5) Conclusion <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3/19\n",
    "\n",
    "- Read through the chapters of Deep Learning for Computer Vision that pertain to classifying CIFAR-10\n",
    "- Read through Adrian Rosebrock's blog posts and Gurus course(Deep Belief Networks) pertaining to autoencoders\n",
    "- Take note of the different types of image pre-processing, network architecture, hyperparameters, optimizers that you want to go ahead with.\n",
    "- Email Adrian about good resources for finishing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Augmentation: Find out what Adrian Rosebrock does when he handles the CIFAR dataset and use that as a base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network Architecture: Find out what Adrian Rosebrock does when he handles the CIFAR dataset and use that as a base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters: Find out what Adrian Rosebrock does when he handles the CIFAR dataset and use that as a base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers: Find out what Adrian Rosebrock does when he handles the CIFAR dataset and use that as a base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3/20\n",
    "\n",
    "- Start implementing each of the different models you want to implement:\n",
    "    - 1:00 PM - 3:00 PM\n",
    "    - Evaluate their performance.\n",
    "- Implement Different Models based on your experiences with the previous ones\n",
    "    - 3:00 PM - 5:00 PM\n",
    "- Do three sections of the GRE\n",
    "    - 5:00 PM - 6:00 PM \n",
    "- Shower and eat \n",
    "    - 6:00 - 7:30\n",
    "- Finalize the models and the architecture you will use to compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3/21\n",
    "\n",
    "- Write the Dataset Explanation\n",
    "    - 8:00 - 11:00\n",
    "    - Explain the number of images\n",
    "    - Class labels\n",
    "    - Particular challenges surrounding this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3/22\n",
    "\n",
    "- Model Architecture/Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3/23 \n",
    "\n",
    "- Explanation/Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3/24\n",
    "\n",
    "-Discussion \n",
    "-Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3/25\n",
    "\n",
    "-README\n",
    "-Proofreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "<details>\n",
    "\n",
    "## Project title\n",
    "\n",
    "The aim of this project is to evaluate a simple hypothesis:\n",
    "    \n",
    "\"Can an Autoencoder + Convolutional Neural Network approach perform better than a simple Convolutional Neural Network classifier?\"\n",
    "\n",
    "Autoencoders are neural networks that traditionally look to encode raw pixel data into a few nodes of a hidden layer, and then decode, or reconstruct the image from that 'compression'     \n",
    "\n",
    "The idea behind the hypothesis is that, the autoencoder might learn to emphasize key distinguishing features when it is trained. Therefore, if we use the encoded representation of the image as input to our CNN, we might achieve better results than if we used the raw image pixel data as input. \n",
    "    \n",
    "## Motivation\n",
    "\n",
    "The motivation behind this project, was simply to explore what autoencoders can and cannot provide to other algorithms and evaluate their usefulness in deepening any particular algorithm. \n",
    "    \n",
    "Autoencoders are a popular algorithm for beginner deep learning practitioners because the idea is easy to understand, and it has a aesthetic appeal to it where doing something so simple might yield benefits to a classification algorithm in question. \n",
    "    \n",
    "However, as with any other hypothesis, we must evaluate it without bias, using the most important performance metrics (precision, accuracy, f-score, confusion matrix) to determine the efficacy. Mechanistic speculation of the algorithm cannot lead to more objective conclusions, which is why we are testing the algorithm today.\n",
    "    \n",
    " \n",
    "## Table Of Contents\n",
    "    \n",
    " - Overview\n",
    " - Requirements\n",
    " - Directory Structure\n",
    " - Tests\n",
    " - Code Example\n",
    " - References\n",
    " - Further Avenues to possibly pursue with this code\n",
    "   \n",
    "## Overview\n",
    "\n",
    "We'll be exploring 5 different models and their performance on CIFAR-10 in this project:\n",
    "    \n",
    " - Convolutional Autoencoder \n",
    " - MiniVGGNet\n",
    " - ShallowNet\n",
    " - Encoder + MiniVGGNet\n",
    " - Encoder + ShallowNet\n",
    "    \n",
    "In the case of the Convolutional Autoencoder, we'll be testing it's ability to recreate images from the CIFAR-10 dataset, and based on those results we'll use the encoder layers as inputs for both MiniVGGNet and ShallowNet. For the rest, we'll be evaluating the model on their ability to correctly classify images in our test set.\n",
    "    \n",
    "## Requirements\n",
    "\n",
    " The relevant libraries are listed below: and their versions are listed below. I used a conda virtual environment to run this project, so all the installation instructions will be presumed to be in a conda environment.\n",
    "- conda 4.8.3\n",
    "- conda-build 3.18.8\n",
    "- python 3.7.6\n",
    "- tensorflow-gpu 2.1 (tensorflow 2.1 should also work here just fine)\n",
    "- matplotlib 3.1.3\n",
    "- numpy 1.18.1    \n",
    "- pandas 1.02\n",
    "- scikit-learn 0.21.3\n",
    "- argparse 1.3.0 \n",
    "- opencv 4.2.0\n",
    "- seaborn 0.10.0\n",
    "\n",
    "You can install Anaconda from https://www.anaconda.com/distribution/, and then run the following commands:\n",
    "    \n",
    "```python\n",
    "conda install -c conda-forge python=3.7.6\n",
    "conda install -c anaconda tensorflow=2.1\n",
    "conda install -c conda-forge matplotlib=3.1.3 \n",
    "conda install -c conda-forge numpy=1.18.1\n",
    "conda install -c anaconda pandas=1.02\n",
    "conda install scikit-learn=0.21,3\n",
    "conda install -c anaconda argparse=1.3.0\n",
    "conda install -c conda-forge opencv=4.2.0\n",
    "conda install -c anaconda seaborn=0.10.0\n",
    "```\n",
    "    \n",
    "## Code style\n",
    "\n",
    "If you're using any code style like xo, standard etc. That will help others while contributing to your project. Ex. -\n",
    "\n",
    "js-standard-style\n",
    "## Screenshots\n",
    "\n",
    "Include logo/demo screenshot etc.\n",
    "Tech/framework used\n",
    "\n",
    "Ex. -\n",
    "\n",
    "## Built with\n",
    "\n",
    "    Electron\n",
    "\n",
    "## Features\n",
    "\n",
    "What makes your project stand out?\n",
    "Code Example\n",
    "\n",
    "Show what the library does as concisely as possible, developers should be able to figure out how your project solves their problem by looking at the code example. Make sure the API you are showing off is obvious, and that your code is short and concise.\n",
    "Installation\n",
    "\n",
    "Provide step by step series of examples and explanations about how to get a development env running.\n",
    "## API Reference\n",
    "\n",
    "Depending on the size of the project, if it is small and simple enough the reference docs can be added to the README. For medium size to larger projects it is important to at least provide a link to where the API reference docs live.\n",
    "## Tests\n",
    "\n",
    "Describe and show how to run the tests with code examples.\n",
    "How to use?\n",
    "\n",
    "If people like your project they’ll want to learn how they can use it. To do so include step by step guide to use your project.\n",
    "## Contribute\n",
    "\n",
    "Let people know how they can contribute into your project. A contributing guideline will be a big plus.\n",
    "Credits\n",
    "\n",
    "Give proper credits. This could be a link to any repo which inspired you to build this project, any blogposts or links to people who contrbuted in this project.\n",
    "Anything else that seems useful\n",
    "## License\n",
    "\n",
    "A short snippet describing the license (MIT, Apache etc)\n",
    "\n",
    "MIT © Yourname\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3/23\n",
    "\n",
    "Start writing the Introduction and the Dataset Explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "### Data augmentation: \n",
    "[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutionalneural networks.  InAdvances in neural information processing systems, pages 1097–1105,2012.<br>\n",
    "[2] M. Paschali, W. Simson, A. G. Roy, M. F. Naeem, R. Göbl, C. Wachinger, and N. Navab. Dataaugmentation with manifold exploring geometric transformations for increased performanceand robustness.arXiv preprint arXiv:1901.04420, 2019.\n",
    "\n",
    "### ShallowNet and ImageNet Representations\n",
    "[3] A. Rosebrock, Deep Learning for Computer Vision, PyImageSearch, https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/, accessed on 24 March 2020, pp 208-210, 231-241.\n",
    "\n",
    "### Autoencoder Representation\n",
    "[4] R. Flynn, Convolutional Autoencoders for the Cifar10 Dataset, https://github.com/rtflynn/Cifar-Autoencoder, accessed on 24 March 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory Structure\n",
    "\n",
    "```bash\n",
    "C:.\n",
    "└───Models\n",
    "    │   convautoencoder_cifar10.py\n",
    "    │   convautoencoder_minivggnet_cifar10.py\n",
    "    │   convautoencoder_shallownet_cifar10.py\n",
    "    │   minivggnet_cifar10.py\n",
    "    │   shallownet_cifar10.py\n",
    "    │\n",
    "    ├───modelcollection\n",
    "    │   ├───callbacks\n",
    "    │   ├───nn\n",
    "    │   │   └───conv\n",
    "    │   │        │   convautoencoder.py\n",
    "    │   │        │   convautoencoder_minivggnet.py\n",
    "    │   │        │   convautoencoder_shallownet.py\n",
    "    │   │        │   minivggnet.py\n",
    "    │   │        └───shallownet.py \n",
    "    │   ├───plot\n",
    "    │   └───preprocessing\n",
    "    │\n",
    "    └───output\n",
    "        ├───plots\n",
    "        │   ├───convautoencoder_minivggnet\n",
    "        │   ├───convautoencoder_shallownet\n",
    "        │   ├───conveautoencoder\n",
    "        │   ├───minivggnet\n",
    "        │   └───shallownet\n",
    "        └───weights\n",
    "            ├───convautoencoder_minivggnet\n",
    "            ├───convautoencoder_shallownet\n",
    "            ├───conveautoencoder\n",
    "            ├───minivggnet\n",
    "            └───shallownet\n",
    "```\n",
    "\n",
    "The main scripts are in the Models folder, with each of the models having it's own script. \n",
    "\n",
    "Each of the respective models' classes are in Models/modelcollection/nn/conv if you need to take a look at or alter the structure of the networks.\n",
    "\n",
    "Each of the models output their relevant performance metricsinto their respective folders\n",
    "in Models/output/plots, as well as the best performing model weights into Model/output/weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Autoencoders are neural networks that seek to encode data into a latent-space representation, and decode this data to obtain emphasized features. Being conceptually easy to explain, they are notorious for being one of the first networks that practitioners new to the deep learning field learn, holding appeal through it's idealistic simplicity. \n",
    "\n",
    "Autoencoders have had various applications over the years, ranging from dimensionality reduction to denoising image datasets. Our aim today, is to use it's most traditional function as an encoder and attempt to answer the question: \"Can an Autoencoder + Convolutional Neural Network approach perform better than a simple Convolutional Neural Network classifier on a classification task?\"\n",
    "\n",
    "The main idea of this experiment is to train an encoder, use the layers up till the latent space representation as input for the Convolutional Neural Network, and see if and how it outperforms a simple Convolutional Neural Network approach. We will be looking at the performance of the Autoencoder, some CNN models, and the Autoencoder + the aforementioned CNN models on the CIFAR-10 dataset. \n",
    "\n",
    "In particular, we will demonstrate how adding the autoencoder layer to a shallow network such as the aptly-named ShallowNet can increase classification accuracy, but when applied to deeper networks such as MiniVGGNet actually can reduce classification accuracy compared to more classical approaches. It is concluded that this is because shallow networks do not have the depth to extract features, which autoencoders help to provide. In addition, established deeper architectures have specific methods to highlight relevant features, while autoencoders may break away from these methods to highlight different features that are not necessarily useful for the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Explanation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our experiment, we are going to employ one of the standard datasets used to evaluate image classification networks, CIFAR-10. \n",
    "\n",
    "There are a few notable qualities about this dataset:\n",
    "\n",
    "First, it has 50,000 training images, and 10,000 testing images, with an even split between 10 classes each, giving us 5,000 training images per class and 1,000 testing images per class. While this may not be in the millions like ImageNet, it has an even split between classes to ensure that no one class is trained significantly more than the other.\n",
    "\n",
    "It should be noted that for this particular experiment, we have the constraint to use only 50% of training data for the following three classes: bird, deer, and truck. Other classes maintain their full amount of training data.\n",
    "\n",
    "Next, the images themselves are 32 x 32 with 3 channels (RGB). This is a very small amount of data for each image, which makes it difficult to get high accuracy.\n",
    "\n",
    "Finally, the images, depending on the class, can feature a significant amount of deformation, occlusion, viewpoint variation, illumination variation, scale variation, and intra-class variation. This not only makes it difficult to gain high accuracy, but also makes the generalizability of the model suffer in question. This means we need both robust architecture and the application of regularization techniques to assist generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model/Architecture explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our goal is to evaluate whether or not an autoencoder approach works better on simple CNN networks, we will be evaluating the classification task on the CIFAR-10 dataset with two less complex models: ShallowNet(with just 1 CONV and FC layer each) and MiniVGGNet[3]. Our choice of models allows us to evaluate the effectiveness of the autoencoder on the simplest of CNNs possible (ShallowNet) as a baseline, then evaluate it on a model with more complexity. Both of these networks will be evaluated on performance with and without the encoder as input.\n",
    "\n",
    "As VGGNet is usually evaluated on ImageNet, and is traditionally either 16 or 19 layers, we will be using a more compact version of the model(6 layers) instead to work with our smaller dataset[3]. The reasoning behind choosing MiniVGGNet over another similar depth network is because it employs multiple CONV => RELU before the POOL layer, and doing so allows the model to discern richer features. If the performance of the network still improves with the autoencoder, that means the latent-space representation contains a great amount of relevant information for the convolutional layers to use in discerning features. \n",
    "\n",
    "The autoencoder we are going to use for our model is a purely convolutional model (no fully connected layers) that employs shallow, strided convolutions in place of max pooling, with batch normalization. It has been demonstrated to accurately replicate images[4] from CIFAR-10 with no noticeable differences between the actual image data and the representations from the autoencoder. We will train this model separately, take the trained encoder and apply it to each of the aforementioned models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests \n",
    "\n",
    "All model outputs will be created in the default folders unless otherwise specified by command line arguments.\n",
    "\n",
    "### Convolutional Autoencoder\n",
    "\n",
    "This test should be run first, as we are training the autoencoder separately from the rest of the CNN model. Therefore, to run the Autoencoder + CNN models we need an existing autoencoder model file to exist.\n",
    "\n",
    "```python\n",
    "python convautoencoder_cifar10.py\n",
    "```\n",
    "<details>\n",
    "<summary>Optional Arguments</summary>\n",
    "<br>\n",
    "\n",
    "- --samples\n",
    "- number of samples to visualize when decoding, \n",
    "- default:8\n",
    "<br>\n",
    "    \n",
    "- --image\n",
    "- path to output image comparison file \n",
    "- default=\"output/plots/conveautoencoder/autoencoder_only_output.png\"\n",
    "<br>\n",
    "\n",
    "- --output\n",
    "- path to output plot file\n",
    "- default=\"output/plots/conveautoencoder/autoencoder_only_plot.png\"\n",
    "<br>\n",
    "\n",
    "- --weights\n",
    "- path to best model weights file\n",
    "- default = 'output/weights/conveautoencoder/convautoencoder_cifar10_best_weights.hdf5'\n",
    "\n",
    "</details>\n",
    "<details>\n",
    "<summary>Outputs</summary>\n",
    "<br>\n",
    "\n",
    "- Image Output Comparison -> output/plots/conveautoencoder/autoencoder_only_output.png\n",
    "- Training and Validation Loss Plot -> output/plots/conveautoencoder/autoencoder_only_plot.png\n",
    "- Best Model (Lowest Validation Loss) -> output/weights/conveautoencoder/convautoencoder_cifar10_best_weights.hdf5\n",
    "    \n",
    "</details>\n",
    "\n",
    "### Convolutional Autoencoder + MiniVGGNet\n",
    "\n",
    "```python\n",
    "python convautoencoder_minivggnet_cifar10.py\n",
    "```\n",
    "<details>\n",
    "<summary>Optional Arguments</summary>\n",
    "<br>\n",
    "\n",
    "- --output\n",
    "- path to the output plot folder\n",
    "- default=\"output/plots/convautoencoder_minivggnet\"\n",
    "<br>\n",
    "\n",
    "- --weights\n",
    "- path to best model weights file\n",
    "- default = 'output/weights/convautoencoder_minivggnet/convautoencoder_minivggnet_cifar10_best_weights.hdf5'\n",
    "<br>\n",
    "    \n",
    "- --autoencoder\n",
    "- path to best autoencoder model weights file\n",
    "- default = 'output/weights/conveautoencoder/convautoencoder_cifar10_best_weights.hdf5'\n",
    "\n",
    "</details>\n",
    "<details>\n",
    "<summary>Outputs</summary>\n",
    "<br>\n",
    "    \n",
    "- Classification Report -> output/plots/convautoencoder_minivggnet/cifar10_convautoencoder_minivggnet_classification_report.png\n",
    "- Confusion Matrix -> output/plots/convautoencoder_minivggnet/cifar10_convautoencoder_minivggnet_conf_matrix.png\n",
    "- Training and Validation Loss Plot -> output/plots/convautoencoder_minivggnet/cifar10_convautoencoder_minivggnet.png\n",
    "- Best Model (Lowest Validation Loss) -> output/weights/convautoencoder_minivggnet/convautoencoder_minivggnet_cifar10_best_weights.hdf5\n",
    "    \n",
    "</details>\n",
    "\n",
    "### Convolutional Autoencoder + ShallowNet\n",
    "\n",
    "```python\n",
    "python convautoencoder_shallownet_cifar10.py\n",
    "```\n",
    "<details>\n",
    "<summary>Optional Arguments</summary>\n",
    "<br>\n",
    "\n",
    "- --output\n",
    "- path to the output plot folder\n",
    "- default=\"output/plots/convautoencoder_shallownet\"\n",
    "<br>\n",
    "\n",
    "- --weights\n",
    "- path to best model weights file\n",
    "- default = 'output/weights/convautoencoder_shallownet/convautoencoder_shallownet_cifar10_best_weights.hdf5'\n",
    "<br>\n",
    "    \n",
    "- --autoencoder\n",
    "- path to best autoencoder model weights file\n",
    "- default = 'output/weights/conveautoencoder/convautoencoder_cifar10_best_weights.hdf5'\n",
    "\n",
    "</details>\n",
    "<details>\n",
    "<summary>Outputs</summary>\n",
    "<br>\n",
    "    \n",
    "- Classification Report -> output/plots/convautoencoder_shallownet/cifar10_convautoencoder_shallownet_classification_report.png\n",
    "- Confusion Matrix -> output/plots/convautoencoder_shallownet/cifar10_convautoencoder_shallownet_conf_matrix.png\n",
    "- Training and Validation Loss Plot -> output/plots/convautoencoder_shallownet/cifar10_convautoencoder_shallownet.png\n",
    "- Best Model (Lowest Validation Loss) -> output/weights/convautoencoder_shallownet/convautoencoder_shallownet_cifar10_best_weights.hdf5\n",
    "    \n",
    "</details>\n",
    "\n",
    "### MiniVGGNet\n",
    "\n",
    "```python\n",
    "python minivggnet_cifar10.py\n",
    "```\n",
    "<details>\n",
    "<summary>Optional Arguments</summary>\n",
    "<br>\n",
    "  \n",
    "- --output\n",
    "- path to the output plot folder\n",
    "- default= \"output/plots/minivggnet\"\n",
    "<br>\n",
    "\n",
    "- --weights\n",
    "- path to best model weights file\n",
    "- default = 'output/weights/minivggnet/minivggnet_cifar10_best_weights.hdf5'\n",
    "\n",
    "</details>\n",
    "<details>\n",
    "<summary>Outputs</summary>\n",
    "<br>\n",
    "    \n",
    "- Classification Report -> output/plots/minivggnet/cifar10_minivggnet_classification_report.png\n",
    "- Confusion Matrix -> output/plots/minivggnet/cifar10_minivggnet_conf_matrix.png\n",
    "- Training and Validation Loss Plot -> output/plots/minivggnet/cifar10_minivggnet.png\n",
    "- Best Model (Lowest Validation Loss) -> output/weights/minivggnet/minivggnet_cifar10_best_weights.hdf5\n",
    "    \n",
    "</details>\n",
    "\n",
    "### ShallowNet\n",
    "\n",
    "```python\n",
    "python shallownet_cifar10.py\n",
    "```\n",
    "<details>\n",
    "<summary>Optional Arguments</summary>\n",
    "<br>\n",
    "    \n",
    "- --output\n",
    "- path to the output plot folder\n",
    "- default= \"output/plots/shallownet\"\n",
    "<br>\n",
    "\n",
    "- --weights\n",
    "- path to best model weights file\n",
    "- default = 'output/weights/shallownet/shallownet_cifar10_best_weights.hdf5'\n",
    "\n",
    "</details>\n",
    "<details>\n",
    "<summary>Outputs</summary>\n",
    "<br>\n",
    "    \n",
    "- Classification Report -> output/plots/shallownet/cifar10_shallownet_classification_report.png\n",
    "- Confusion Matrix -> output/plots/shallownet/cifar10_shallownet_conf_matrix.png\n",
    "- Training and Validation Loss Plot -> output/plots/shallownet/cifar10_shallownet.png\n",
    "- Best Model (Lowest Validation Loss) -> output/weights/shallownet/shallownet_cifar10_best_weights.hdf5\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
